get_ipython().run_cell_magic("sh", "", """pip install sagemaker langchain amazon-textract-caller amazon-textract-textractor sentence-transformers pypdf pip install faiss-cpu -qU""")


import boto3, json, sagemaker
from typing import Dict
from langchain import LLMChain
from langchain.docstore.document import Document
from langchain.prompts import PromptTemplate
from langchain.llms import SagemakerEndpoint
from langchain.llms.sagemaker_endpoint import LLMContentHandler
from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri


#t5 XL
# Hub Model configuration. https://huggingface.co/models
role = sagemaker.get_execution_role()

hub = {
	#'HF_MODEL_ID':'google/flan-t5-small',
    'HF_MODEL_ID':'google/flan-t5-xl',
    #'HF_MODEL_ID':'google/flan-t5-xxl',
	'SM_NUM_GPUS': json.dumps(1)
}



# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
	image_uri=get_huggingface_llm_image_uri("huggingface",version="1.1.0"),
	env=hub,
	role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
	initial_instance_count=1,
	instance_type="ml.g4dn.2xlarge",
	container_startup_health_check_timeout=300,
    endpoint_name="flan-t5-demo"
  )
  
# send request
predictor.predict({
	"inputs": "Translate to German:  My name is Arthur",
})


endpoint_name = predictor.endpoint_name
endpoint_name


question = "Which instances can I use with Managed Spot Training in SageMaker?"

out = predictor.predict({"inputs": question})
out


context = """Managed Spot Training can be used with all instances
supported in Amazon SageMaker. Managed Spot Training is supported
in all AWS Regions where Amazon SageMaker is currently available."""


prompt_template = """Answer the following QUESTION based on the CONTEXT
given. #If you do not know the answer and the CONTEXT doesn't
contain the answer truthfully say "I don't know".

CONTEXT:
{context}

QUESTION:
{question}

ANSWER:
"""

text_input = prompt_template.replace("{context}", context).replace("{question}", question)

out = predictor.predict({"inputs": text_input})
generated_text = out[0]["generated_text"]
print(f"[Input]: {question}\n[Output]: {generated_text}")


unanswerable_question = "What color is my desk?"

text_input = prompt_template.replace("{context}", context).replace("{question}", unanswerable_question)

out = predictor.predict({"inputs": text_input})
generated_text = out[0]["generated_text"]
print(f"[Input]: {unanswerable_question}\n[Output]: {generated_text}")


model_kwargs = {"max_new_tokens": 512,
    "top_p": 0.8, "temperature": 0.8,"max_length": 512,"min_length":300 }


#define content handler class
class ContentHandler(LLMContentHandler):
    content_type = "application/json"
    accepts = "application/json"

    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:
        input_data = {
            "inputs": prompt,  # Adjust this field based on the expected input format
            **model_kwargs,
        }
        input_str = json.dumps(input_data)
        return input_str.encode("utf-8")
     
    def transform_output(self, output: 'StreamingBody') -> str:
            response_json = json.loads(output.read().decode("utf-8"))
            print(response_json)
            return response_json[0]["generated_text"]


content_handler = ContentHandler()



import boto3
sm_client = boto3.client("sagemaker-runtime") # needed for AWS credentials

llm = SagemakerEndpoint(
    endpoint_name=endpoint_name,
    model_kwargs=model_kwargs,
    content_handler=content_handler,
    client=sm_client,
)




system_prompt = """Answer the following QUESTION based on the CONTEXT
given. If you do not know the answer and the CONTEXT doesn't
contain the answer truthfully say "I don't know".

CONTEXT:
{context}

QUESTION:
{question}

ANSWER:
"""

prompt = PromptTemplate.from_template(system_prompt + "{context}")


llm_chain = LLMChain(llm=llm, prompt=prompt)


context ="Solar investments trends in China have increased by 30% each year in the last decade"
question = "What is the investment trend for solar investments in China?"

query = f"question: {question}"


query = f"question: {question}"
print(query)


answer = llm_chain.run({"context": context, "question": query})
print(answer)


from langchain.document_loaders import AmazonTextractPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFDirectoryLoader


#!pip install pypdf
#!pip install pypdf2


#working fine to load multiple files
import boto3
import tempfile
from io import BytesIO
from langchain.document_loaders import PyPDFLoader
import os

# Initialize Boto3 S3 client
s3 = boto3.client('s3')
bucket_name = "bo-automation1"
prefix = "langchain-rag-demo/"

# List objects within the specified directory
response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)

# Initialize a list to hold all documents
all_docs = []

# Iterate over each file and load its contents
for obj in response.get('Contents', []):
    item_name = obj['Key']
    # Process only PDF files
    if item_name.endswith('.pdf'):
        print(f"Loading file: {item_name}")

        # Get the PDF file content from S3
        pdf_response = s3.get_object(Bucket=bucket_name, Key=item_name)
        pdf_content = pdf_response["Body"].read()

        # Use BytesIO to create a file-like object from the PDF content
        pdf_file = BytesIO(pdf_content)

        # Save the contents to a temporary file
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file.write(pdf_content)
            temp_file_path = temp_file.name

        # Load the PDF document
        loader = PyPDFLoader(temp_file_path)
        docs = loader.load()
        all_docs.extend(docs)
        print(len(docs))

        # Optionally, delete the temporary file
        os.remove(temp_file_path)

# Now all_docs contains documents from all PDF files



print(len(all_docs))


#working fine
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Split documents into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)
all_chunks = []

# Assuming 'docs' is the list of loaded documents
for document in all_docs:
    # Extract text from the document
    #text = document.content  # Adjust this based on the actual structure of your Document object

    # Split the text into chunks
    chunks = text_splitter.split_documents(all_docs)

    # Add the chunks to the list
    all_chunks += chunks

    # Print information about the chunks
    print(f"Original text length: {len(all_docs)}, number of chunks: {len(chunks)}")

#chunks = text_splitter.split_documents(docs)

# Print information about the chunks
#print(f"Original text length: {len(docs)}, number of chunks: {len(chunks)}")
print(f"Number of chunks: {len(all_chunks)}")



#from langchain.document_loaders import AmazonTextractPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA


# Define embedding model
# See https://huggingface.co/spaces/mteb/leaderboard

embedding_model_id = "BAAI/bge-small-en-v1.5"

embeddings = HuggingFaceEmbeddings(
    model_name=embedding_model_id,
)


get_ipython().run_cell_magic("time", "", """# Embed chunks
#embeddings_db = FAISS.from_documents(all_chunks, embeddings)
print(f"Number of chunks: {len(all_chunks)}")
#print(f"Number of embeddings: {len(embeddings)}")

# Embed chunks
embeddings_db = FAISS.from_documents(all_chunks, embeddings)
""")


# Save database
embeddings_db.save_local("faiss_index")


embeddings_db = FAISS.load_local("faiss_index", embeddings)


retriever = embeddings_db.as_retriever(search_kwargs={"k": 5})


#working version
# Define prompt template1
system_prompt = """Answer the following QUESTION based on the CONTEXT
given. If you do not know the answer and the CONTEXT doesn't
contain the answer truthfully say "I don't know".

CONTEXT:
{context}

QUESTION:
{question}

ANSWER:
"""

prompt_template = PromptTemplate.from_template(system_prompt + "{context}")


chain = RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type="stuff",
    retriever=retriever, 
    chain_type_kwargs = {"prompt": prompt_template})


question = "System Requirements for Cloud Container Distribution?"
answer = chain.run({"query": question})
#print(answer)

#answer = chain.run({"query": question})
print(answer)



question = "How much raw capacity is available in ceph cluster?"
answer = chain.run({"query": question})
print(answer)


question = "what is the result of encryption?"
answer = chain.run({"query": question})
print(answer)


question = "A CCSM instance can be connected to a maximum of how many HSM?"
answer = chain.run({"query": question})
print(answer)


question = "what are System Requirements for Cloud Container Distributionl"
answer = chain.run({"query": question})
print(answer)


#pip install gradio
#import gradio
import pydantic

#print("Gradio version:", gradio.__version__)
print("Pydantic version:", pydantic.__version__)



pip install --upgrade pydantic gradio


pip install --upgrade pydantic gradio


pip install gradio==3.48.0


pip install --upgrade starlette


import gradio as gr


# Assuming the initialization of your model, prompt template, and chain is done here
# Define prompt template
system_prompt = """Answer the following QUESTION based on the CONTEXT
given. If you do not know the answer and the CONTEXT doesn't
contain the answer truthfully say "I don't know".

CONTEXT:
{context}

QUESTION:
{question}

ANSWER:
"""

prompt_template = PromptTemplate.from_template(system_prompt + "{context}")

chain = RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type="stuff",
    retriever=retriever, 
    chain_type_kwargs = {"prompt": prompt_template}
)

# Define gradio model function
def model_function(question):
    # Make prediction using the chain
    answer = chain.run({"query": question})
    return answer




# Define the gradio interface for our use case
interface = gr.Interface(fn=model_function, inputs="text", outputs="text")
interface.launch()
#interface.launch(share=True)




import gradio as gr

# Define the gradio model function (assuming model_function is already defined)
def model_function(question):
    answer = chain.run({"query": question})
    return answer

# Define the gradio interface with a title and description
interface = gr.Interface(
    fn=model_function,
    inputs=gr.Textbox(label="Enter your question here"),
    outputs="text",
    title="Test system for Bid office SoC Answering System",
    description="This is a simple question answering system. Upload a document, ask your question, and get an answer."
)

# Launch the interface
interface.launch()
# Use interface.launch(share=True) if you want to share your app with others



predictor.delete_model()
predictor.delete_endpoint()



