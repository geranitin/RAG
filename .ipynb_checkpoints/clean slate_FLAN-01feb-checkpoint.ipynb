{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e561e1b6",
   "metadata": {},
   "source": [
    "# Building a simple RAG chatbot with LangChain, Hugging Face, FAISS, Amazon SageMaker and Amazon Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e7b807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.1 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pip install sagemaker langchain amazon-textract-caller amazon-textract-textractor sentence-transformers pypdf pip install faiss-cpu -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91613d24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, sagemaker\n",
    "from typing import Dict\n",
    "from langchain import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ff459",
   "metadata": {},
   "source": [
    "## Deploy LLM on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4f57eb-d660-41f3-901a-ce93d32ae874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Ich bin Arthur.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t5 XL\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "\t#'HF_MODEL_ID':'google/flan-t5-small',\n",
    "    'HF_MODEL_ID':'google/flan-t5-xl',\n",
    "    #'HF_MODEL_ID':'google/flan-t5-xxl',\n",
    "\t'SM_NUM_GPUS': json.dumps(1)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g4dn.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "    endpoint_name=\"flan-t5-demo\"\n",
    "  )\n",
    "  \n",
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"Translate to German:  My name is Arthur\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6bbbb01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flan-t5-demo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90184f3-87f8-47fd-ad88-11138a52bd9b",
   "metadata": {},
   "source": [
    "**Zero Shot example** 1. Ask a question to LLM without providing the context\n",
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. Let's directly ask the model a question and see how they respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b4361a-d97f-466f-a179-6c4d71248d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'SageMaker and SageMaker XL.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "\n",
    "out = predictor.predict({\"inputs\": question})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41391e07-bf7a-4806-ab3b-1fda2ba25be3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# define payload\n",
    "prompt=\"\"\"<|prompter|>How can i stay more active during winter? Give me 3 tips.<|endoftext|><|assistant|>\"\"\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"min_length\": 200,\n",
    "    \"stop\": [\"<|endoftext|>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "# print(response[0][\"generated_text\"][:-len(\"<human>:\")])\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ee932-209d-4c0d-849a-2f7413d58a23",
   "metadata": {},
   "source": [
    "Step 3. Improve the answer to the same question using prompt engineering with insightful context\n",
    "To better answer the question well, we provide extra contextual information, combine it with a prompt, and send it to model together with the question. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "482f3ca1-9ef3-41e7-a8bd-1f2fab9fdb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Managed Spot Training can be used with all instances\n",
    "supported in Amazon SageMaker. Managed Spot Training is supported\n",
    "in all AWS Regions where Amazon SageMaker is currently available.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f28f04-7ce8-4ced-9594-77ab72066ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: Which instances can I use with Managed Spot Training in SageMaker?\n",
      "[Output]: all instances supported in Amazon SageMaker\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. #If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "text_input = prompt_template.replace(\"{context}\", context).replace(\"{question}\", question)\n",
    "\n",
    "out = predictor.predict({\"inputs\": text_input})\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "print(f\"[Input]: {question}\\n[Output]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c6964-4c87-4134-8be0-6d31371094eb",
   "metadata": {},
   "source": [
    "Let's see if our LLM is capable of following our instructions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602bdd95-42b4-477a-a081-1802d6f81512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: What color is my desk?\n",
      "[Output]: I don't know\n"
     ]
    }
   ],
   "source": [
    "unanswerable_question = \"What color is my desk?\"\n",
    "\n",
    "text_input = prompt_template.replace(\"{context}\", context).replace(\"{question}\", unanswerable_question)\n",
    "\n",
    "out = predictor.predict({\"inputs\": text_input})\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "print(f\"[Input]: {unanswerable_question}\\n[Output]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3547b7",
   "metadata": {},
   "source": [
    "## Configure LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4ef122-30de-4baf-8162-cc2eab768da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 1024,\n",
    "    \"top_p\": 0.8, \"temperature\": 0.8,\"max_length\": 512,\"min_length\":300 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d559a5-64cc-4c49-9faf-82f9a75aa910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define content handler class\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_data = {\n",
    "            \"inputs\": prompt,  # Adjust this field based on the expected input format\n",
    "            **model_kwargs,\n",
    "        }\n",
    "        input_str = json.dumps(input_data)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "     \n",
    "    def transform_output(self, output: 'StreamingBody') -> str:\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            print(response_json)\n",
    "            return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fae0bb69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker-runtime\") # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=sm_client,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a7cb9",
   "metadata": {},
   "source": [
    "## Zero-shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80b8fd9d-0fad-4184-a680-b477da384ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(system_prompt + \"{context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28e5f4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77e58740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context =\"Solar investments trends in China have increased by 30% each year in the last decade\"\n",
    "question = \"What is the investment trend for solar investments in China?\"\n",
    "\n",
    "query = f\"question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "803ac882-ba65-462d-bba7-9388ac60cd92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is the investment trend for solar investments in China?\n"
     ]
    }
   ],
   "source": [
    "query = f\"question: {question}\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ba91871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '30%'}]\n",
      "30%\n"
     ]
    }
   ],
   "source": [
    "answer = llm_chain.run({\"context\": context, \"question\": query})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45079dc5",
   "metadata": {},
   "source": [
    "## RAG example with PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19eb8f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ded1aba-4832-448a-982f-942bcdff6933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install pypdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e586d1-21c6-4daf-8db1-87b69ad69ccb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#this is working for single pdf it is created as pypdf can't load s3 object directly\n",
    "import boto3\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Specify your S3 bucket name and item name\n",
    "bucket_name = \"bo-automation\"\n",
    "item_name = \"langchain-rag-demo/Coal2022.pdf\"\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Get the PDF file content from S3\n",
    "response = s3.get_object(Bucket=bucket_name, Key=item_name)\n",
    "pdf_content = response[\"Body\"].read()\n",
    "\n",
    "# Use BytesIO to create a file-like object from the PDF content\n",
    "pdf_file = BytesIO(pdf_content)\n",
    "\n",
    "# Save the contents to a temporary file\n",
    "with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "    temp_file.write(pdf_content)\n",
    "    temp_file_path = temp_file.name\n",
    "\n",
    "# Create a PyPDFLoader instance and load the PDF document\n",
    "loader = PyPDFLoader(temp_file_path)\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "\n",
    "# Now you can work with the 'document' object, which represents the PDF content\n",
    "# For example, you can access the pages: document.pages\n",
    "\n",
    "# Optionally, delete the temporary file\n",
    "os.remove(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35deeb8a-0073-464e-84d2-16d4170e773a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: langchain-rag-demo/5G_Dimensioning and Network Design Guidelines.docx.pdf\n",
      "91\n",
      "Loading file: langchain-rag-demo/dimensioning_guide.pdf\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#working fine to load multiple files\n",
    "import boto3\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Initialize Boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = \"bo-automation1\"\n",
    "prefix = \"langchain-rag-demo/\"\n",
    "\n",
    "# List objects within the specified directory\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# Initialize a list to hold all documents\n",
    "all_docs = []\n",
    "\n",
    "# Iterate over each file and load its contents\n",
    "for obj in response.get('Contents', []):\n",
    "    item_name = obj['Key']\n",
    "    # Process only PDF files\n",
    "    if item_name.endswith('.pdf'):\n",
    "        print(f\"Loading file: {item_name}\")\n",
    "\n",
    "        # Get the PDF file content from S3\n",
    "        pdf_response = s3.get_object(Bucket=bucket_name, Key=item_name)\n",
    "        pdf_content = pdf_response[\"Body\"].read()\n",
    "\n",
    "        # Use BytesIO to create a file-like object from the PDF content\n",
    "        pdf_file = BytesIO(pdf_content)\n",
    "\n",
    "        # Save the contents to a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "            temp_file.write(pdf_content)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        # Load the PDF document\n",
    "        loader = PyPDFLoader(temp_file_path)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(len(docs))\n",
    "\n",
    "        # Optionally, delete the temporary file\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "# Now all_docs contains documents from all PDF files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a38dfb43-ae83-4355-8c37-56369ad7cb99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "print(len(all_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef19880-00ae-479c-8e8a-6f57b8e73348",
   "metadata": {
    "tags": []
   },
   "source": [
    "#not using,but working fine\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Print information about the chunks\n",
    "print(f\"Original text length: {len(docs)}, number of chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7250dfde-4f8e-47cc-ae87-803fec118093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Number of chunks: 94615\n"
     ]
    }
   ],
   "source": [
    "#working fine\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "all_chunks = []\n",
    "\n",
    "# Assuming 'docs' is the list of loaded documents\n",
    "for document in all_docs:\n",
    "    # Extract text from the document\n",
    "    #text = document.content  # Adjust this based on the actual structure of your Document object\n",
    "\n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "    # Add the chunks to the list\n",
    "    all_chunks += chunks\n",
    "\n",
    "    # Print information about the chunks\n",
    "    print(f\"Original text length: {len(all_docs)}, number of chunks: {len(chunks)}\")\n",
    "\n",
    "#chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Print information about the chunks\n",
    "#print(f\"Original text length: {len(docs)}, number of chunks: {len(chunks)}\")\n",
    "print(f\"Number of chunks: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745175",
   "metadata": {},
   "source": [
    "### Analyze documents with Amazon Textract and split them in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c74a8",
   "metadata": {},
   "source": [
    "### Embed document chunks and store them in FAISS\n",
    "https://github.com/facebookresearch/faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908d463e-445b-49eb-b5c9-539c790047fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa50471b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9be476830df4eac8fc7bb79cc0f6895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f09f318c4434f0c9759c19482d2ec35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a38f3fc13f043fcad7dba15ed69eb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8955b82de24c3f93d6ed06aabd3146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e83ce49b24340b8bef36abebeb53fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e279071002743a99bb850702ad1b883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6894b528c8741e59cd8a30336176e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2941ad38fe73447ca032bc399e47e366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aa79f613544623ae5920aece3a8635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fafab141de4ce7b54931b093df7033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b0a23d55cd434481ada04f86a0340f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define embedding model\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c61d096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 94615\n",
      "CPU times: user 2min 23s, sys: 2.66 s, total: 2min 25s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed chunks\n",
    "#embeddings_db = FAISS.from_documents(all_chunks, embeddings)\n",
    "print(f\"Number of chunks: {len(all_chunks)}\")\n",
    "#print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "# Embed chunks\n",
    "embeddings_db = FAISS.from_documents(all_chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eda69b-0567-41e7-8475-3c699f9b18fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#not using\n",
    "%%time\n",
    "#alternate\n",
    "# Embed chunks\n",
    "#embeddings_db = FAISS.from_documents(all_chunks, embeddings)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "#print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "# Embed chunks\n",
    "embeddings_db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f3cbb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save database\n",
    "embeddings_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb455a",
   "metadata": {},
   "source": [
    "### Shortcut : load existing embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e86b0ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22850",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4d47",
   "metadata": {},
   "source": [
    "### Configure RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4148a333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = embeddings_db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e503141b-56d8-4e24-bea7-6f97b9443007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#working version\n",
    "# Define prompt template1\n",
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(system_prompt + \"{context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a92ef004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt_template})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06143a",
   "metadata": {},
   "source": [
    "### Ask our question again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64223366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '2 2 Dimensioning When Using Image-based Deployment 3 3 Dimensioning When '}]\n",
      "2 2 Dimensioning When Using Image-based Deployment 3 3 Dimensioning When \n"
     ]
    }
   ],
   "source": [
    "question = \"System Requirements for Cloud Container Distribution?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "#print(answer)\n",
    "\n",
    "#answer = chain.run({\"query\": question})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8a47bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I don't know\"}]\n",
      "I don't know\n"
     ]
    }
   ],
   "source": [
    "question = \"How much raw capacity is available in ceph cluster?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4bcbacd-e9f5-4169-84ee-60d34e8f1732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'increases the latency and also lowers the maximum load the system can handle. Because of this,'}]\n",
      "increases the latency and also lowers the maximum load the system can handle. Because of this,\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the result of encryption?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9732d7c9-b848-4399-be50-8d3f441db889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '6+1'}]\n",
      "6+1\n"
     ]
    }
   ],
   "source": [
    "question = \"A CCSM instance can be connected to a maximum of how many HSM?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "272eb5bb-901f-4b3f-b1fc-60f4335dabd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The generic requirements to enable installation of Cloud Container Distribution are described in Infrastructure Requirements'}]\n",
      "The generic requirements to enable installation of Cloud Container Distribution are described in Infrastructure Requirements\n"
     ]
    }
   ],
   "source": [
    "question = \"what are System Requirements for Cloud Container Distributionl\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8797d3b-de9f-42ba-8563-1c884ea34a83",
   "metadata": {},
   "source": [
    "Alternate approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc05de-cdb2-4370-bf04-c54104d07aaa",
   "metadata": {},
   "source": [
    "Test interface using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bf98bef-bb38-4212-9315-4b508f3b7782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic version: 1.10.14\n"
     ]
    }
   ],
   "source": [
    "#pip install gradio\n",
    "#import gradio\n",
    "import pydantic\n",
    "\n",
    "#print(\"Gradio version:\", gradio.__version__)\n",
    "print(\"Pydantic version:\", pydantic.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b48b1e18-7afc-422d-a3fc-c828cbb393cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.10.14)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gradio\n",
      "  Downloading gradio-4.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.1 (from pydantic)\n",
      "  Downloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.95.2)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.8.1 (from gradio)\n",
      "  Downloading gradio_client-0.8.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx (from gradio)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (6.1.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (1.22.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.9.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (10.0.1)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio)\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.1.7 (from gradio)\n",
      "  Downloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: uvicorn>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.22.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio-client==0.8.1->gradio) (2023.10.0)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==0.8.1->gradio)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n",
      "Requirement already satisfied: toolz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.4)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.6.0)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "INFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastapi (from gradio)\n",
      "  Downloading fastapi-0.109.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting starlette<0.36.0,>=0.35.0 (from fastapi->gradio)\n",
      "  Downloading starlette-0.35.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (4.0.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx->gradio)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx->gradio) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.18)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\n",
      "Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio-4.16.0-py3-none-any.whl (16.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.8.1-py3-none-any.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.9.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=03f43d2a39628913c46fd98abd4d844ed0c91328a6f0a5bfb960bf09244af931\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, typer, tomlkit, shellingham, semantic-version, ruff, python-multipart, pydantic-core, orjson, httpcore, annotated-types, aiofiles, starlette, pydantic, httpx, gradio-client, fastapi, altair, gradio\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.12.1\n",
      "    Uninstalling tomlkit-0.12.1:\n",
      "      Successfully uninstalled tomlkit-0.12.1\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.27.0\n",
      "    Uninstalling starlette-0.27.0:\n",
      "      Successfully uninstalled starlette-0.27.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.14\n",
      "    Uninstalling pydantic-1.10.14:\n",
      "      Successfully uninstalled pydantic-1.10.14\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.95.2\n",
      "    Uninstalling fastapi-0.95.2:\n",
      "      Successfully uninstalled fastapi-0.95.2\n",
      "Successfully installed aiofiles-23.2.1 altair-5.2.0 annotated-types-0.6.0 fastapi-0.109.0 ffmpy-0.3.1 gradio-4.16.0 gradio-client-0.8.1 httpcore-1.0.2 httpx-0.26.0 orjson-3.9.12 pydantic-2.6.0 pydantic-core-2.16.1 pydub-0.25.1 python-multipart-0.0.6 ruff-0.1.15 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.35.1 tomlkit-0.12.0 typer-0.9.0 websockets-11.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pydantic gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f0c68e8-d96b-4a8f-ba5e-56b16080526a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: gradio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (2.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (5.2.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.109.0)\n",
      "Requirement already satisfied: ffmpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.8.1)\n",
      "Requirement already satisfied: httpx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.26.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (6.1.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (1.22.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.9.12)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (10.0.1)\n",
      "Requirement already satisfied: pydub in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.1.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.1.15)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.22.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio-client==0.8.1->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio-client==0.8.1->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n",
      "Requirement already satisfied: toolz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.4)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.6.0)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->gradio) (0.35.1)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (4.0.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (1.0.2)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx->gradio) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.18)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pydantic gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93d34bf9-98a8-4ed8-bc5f-12ed79f929dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio==3.48.0\n",
      "  Downloading gradio-3.48.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (5.2.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.109.0)\n",
      "Requirement already satisfied: ffmpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.3.1)\n",
      "Collecting gradio-client==0.6.1 (from gradio==3.48.0)\n",
      "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.26.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (6.1.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (1.22.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (3.9.12)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (10.0.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.6.0)\n",
      "Requirement already satisfied: pydub in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (6.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.31.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (4.8.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.22.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (11.0.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio-client==0.6.1->gradio==3.48.0) (2023.10.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.48.0) (4.19.1)\n",
      "Requirement already satisfied: toolz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.48.0) (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio==3.48.0) (3.12.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio==3.48.0) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.48.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.48.0) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.48.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.48.0) (2.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests~=2.0->gradio==3.48.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests~=2.0->gradio==3.48.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests~=2.0->gradio==3.48.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests~=2.0->gradio==3.48.0) (2023.7.22)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.48.0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.48.0) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->gradio==3.48.0) (0.35.1)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio==3.48.0) (4.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio==3.48.0) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio==3.48.0) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.48.0) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx->gradio==3.48.0) (1.1.3)\n",
      "Downloading gradio-3.48.0-py3-none-any.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gradio-client, gradio\n",
      "  Attempting uninstall: gradio-client\n",
      "    Found existing installation: gradio_client 0.8.1\n",
      "    Uninstalling gradio_client-0.8.1:\n",
      "      Successfully uninstalled gradio_client-0.8.1\n",
      "  Attempting uninstall: gradio\n",
      "    Found existing installation: gradio 4.16.0\n",
      "    Uninstalling gradio-4.16.0:\n",
      "      Successfully uninstalled gradio-4.16.0\n",
      "Successfully installed gradio-3.48.0 gradio-client-0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio==3.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f05232d-8997-4191-a5ad-680dcb24ea43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: starlette in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.35.1)\n",
      "Collecting starlette\n",
      "  Downloading starlette-0.36.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from starlette) (4.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette) (1.1.3)\n",
      "Downloading starlette-0.36.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: starlette\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.35.1\n",
      "    Uninstalling starlette-0.35.1:\n",
      "      Successfully uninstalled starlette-0.35.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.109.0 requires starlette<0.36.0,>=0.35.0, but you have starlette 0.36.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed starlette-0.36.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade starlette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e9cb79d-c5cc-4bbd-90c2-2ac71ab2a3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a79a1283-1157-4779-825c-21aa72f594f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.23.20-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.23.9 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.23.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Downloading PyMuPDF-1.23.20-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.23.20 PyMuPDFb-1.23.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54f85134-3888-41f0-9340-101bb318ad11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming the initialization of your model, prompt template, and chain is done here\n",
    "# Define prompt template\n",
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(system_prompt + \"{context}\")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# Define gradio model function\n",
    "def model_function(question):\n",
    "    # Make prediction using the chain\n",
    "    answer = chain.run({\"query\": question})\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f60bc014-cb3d-4aa5-bdb6-feb677d93796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Running on public URL: https://4c39b1da777ebb244f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4c39b1da777ebb244f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The generic requirements to enable installation of Cloud Container Distribution are described in Infrastructure Requirements'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_community/llms/sagemaker_endpoint.py\", line 355, in _call\n",
      "    response = self.client.invoke_endpoint(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py\", line 553, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py\", line 1009, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (422) from primary with message \"{\"error\":\"Input validation error: `inputs` must have less than 1024 tokens. Given: 1176\",\"error_type\":\"validation\"}\". See https://eu-north-1.console.aws.amazon.com/cloudwatch/home?region=eu-north-1#logEventViewer:group=/aws/sagemaker/Endpoints/flan-t5-demo in account 254455524940 for more information.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/routes.py\", line 534, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/blocks.py\", line 1185, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 833, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/utils.py\", line 661, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_9278/1556230268.py\", line 28, in model_function\n",
      "    answer = chain.run({\"query\": question})\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 538, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 363, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 162, in invoke\n",
      "    raise e\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\", line 144, in _call\n",
      "    answer = self.combine_documents_chain.run(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 543, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 363, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 162, in invoke\n",
      "    raise e\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 136, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 363, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 162, in invoke\n",
      "    raise e\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 525, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 698, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 562, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 549, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 1134, in _generate\n",
      "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_community/llms/sagemaker_endpoint.py\", line 363, in _call\n",
      "    raise ValueError(f\"Error raised by inference endpoint: {e}\")\n",
      "ValueError: Error raised by inference endpoint: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (422) from primary with message \"{\"error\":\"Input validation error: `inputs` must have less than 1024 tokens. Given: 1176\",\"error_type\":\"validation\"}\". See https://eu-north-1.console.aws.amazon.com/cloudwatch/home?region=eu-north-1#logEventViewer:group=/aws/sagemaker/Endpoints/flan-t5-demo in account 254455524940 for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I don't know\"}]\n",
      "[{'generated_text': \"I don't know\"}]\n",
      "[{'generated_text': 'a Telco-grade database, designed to'}]\n",
      "[{'generated_text': 'a Telco-grade database, designed to'}]\n"
     ]
    }
   ],
   "source": [
    "# Define the gradio interface for our use case\n",
    "interface = gr.Interface(fn=model_function, inputs=\"text\", outputs=\"text\")\n",
    "interface.launch()\n",
    "#interface.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "836674f9-ccc9-4984-b9e1-183d1f5e92d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Running on public URL: https://dd4d5192428143f38b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dd4d5192428143f38b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/routes.py\", line 534, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/blocks.py\", line 1185, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 833, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/utils.py\", line 661, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_8709/3251888963.py\", line 27, in <lambda>\n",
      "    fn=lambda pdf_file, question: answer_question(extract_text_from_pdf(pdf_file), question),\n",
      "  File \"/tmp/ipykernel_8709/3251888963.py\", line 7, in extract_text_from_pdf\n",
      "    with fitz.open(stream=pdf_file.read(), filetype=\"pdf\") as doc:\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fitz/__init__.py\", line 2669, in __init__\n",
      "    raise EmptyFileError(msg)\n",
      "fitz.EmptyFileError: cannot open empty document\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/routes.py\", line 534, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/blocks.py\", line 1185, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 833, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/gradio/utils.py\", line 661, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_8709/3251888963.py\", line 27, in <lambda>\n",
      "    fn=lambda pdf_file, question: answer_question(extract_text_from_pdf(pdf_file), question),\n",
      "  File \"/tmp/ipykernel_8709/3251888963.py\", line 7, in extract_text_from_pdf\n",
      "    with fitz.open(stream=pdf_file.read(), filetype=\"pdf\") as doc:\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fitz/__init__.py\", line 2669, in __init__\n",
      "    raise EmptyFileError(msg)\n",
      "fitz.EmptyFileError: cannot open empty document\n"
     ]
    }
   ],
   "source": [
    "# test gradio 2\n",
    "import gradio as gr\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize your text splitter with the desired chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=50)\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Function to extract text from PDF\n",
    "    text = \"\"\n",
    "    with fitz.open(stream=pdf_file.read(), filetype=\"pdf\") as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def process_text_into_chunks(text):\n",
    "    # Function to split text into chunks\n",
    "    chunks = text_splitter.split_documents([text])\n",
    "    return chunks\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    # Assuming you have a function to generate embeddings for text chunks\n",
    "    embeddings = [embed_text(chunk) for chunk in chunks]  # Pseudocode\n",
    "    return embeddings\n",
    "\n",
    "def model_function(pdf_file, question):\n",
    "    # Process the PDF and extract text\n",
    "    context = extract_text_from_pdf(pdf_file)\n",
    "    \n",
    "    # Split the context into chunks\n",
    "    chunks = process_text_into_chunks(context)\n",
    "    \n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "    \n",
    "    # Use the RAG model to answer the question based on the embeddings\n",
    "    # This part of the code would depend on how your RAG model uses embeddings to generate an answer\n",
    "    answer = rag_answer_question(question, embeddings)  # Pseudocode\n",
    "    return answer\n",
    "\n",
    "# Gradio Interface\n",
    "interface = gr.Interface(\n",
    "    fn=model_function, \n",
    "    inputs=[gr.File(type=\"file\", label=\"Upload PDF\"), gr.Textbox(label=\"Your Question\")], \n",
    "    outputs=\"text\"\n",
    ")\n",
    "\n",
    "interface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d4337",
   "metadata": {},
   "source": [
    "## Delete endpoint and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faee172-cbfb-4761-9c57-5d792ae5ab65",
   "metadata": {
    "tags": []
   },
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8de28-b64c-40b6-8974-0fd2a6c9d0ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "response = sagemaker.list_endpoints()\n",
    "\n",
    "if not response['Endpoints']:\n",
    "    print(\"No active endpoints.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e58dd-125e-4d7c-9337-540d0ec7ce33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
