{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e561e1b6",
   "metadata": {},
   "source": [
    "# Building a simple RAG chatbot with LangChain, Hugging Face, FAISS, Amazon SageMaker and Amazon Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e7b807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker langchain amazon-textract-caller amazon-textract-textractor sentence-transformers pypdf pip install faiss-cpu -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91613d24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, sagemaker\n",
    "from typing import Dict\n",
    "from langchain import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ff459",
   "metadata": {},
   "source": [
    "## Deploy LLM on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4f57eb-d660-41f3-901a-ce93d32ae874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Ich bin Arthur.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t5 XL\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "\t#'HF_MODEL_ID':'google/flan-t5-small',\n",
    "    'HF_MODEL_ID':'google/flan-t5-xl',\n",
    "\t'SM_NUM_GPUS': json.dumps(1)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g4dn.4xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "    endpoint_name=\"flan-t5-demo\"\n",
    "  )\n",
    "  \n",
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"Translate to German:  My name is Arthur\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6bbbb01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flan-t5-demo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90184f3-87f8-47fd-ad88-11138a52bd9b",
   "metadata": {},
   "source": [
    "Step 2. Ask a question to LLM without providing the context\n",
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. Let's directly ask the model a question and see how they respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b4361a-d97f-466f-a179-6c4d71248d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'SageMaker and SageMaker XL.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "\n",
    "out = predictor.predict({\"inputs\": question})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ee932-209d-4c0d-849a-2f7413d58a23",
   "metadata": {},
   "source": [
    "Step 3. Improve the answer to the same question using prompt engineering with insightful context\n",
    "To better answer the question well, we provide extra contextual information, combine it with a prompt, and send it to model together with the question. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482f3ca1-9ef3-41e7-a8bd-1f2fab9fdb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Managed Spot Training can be used with all instances\n",
    "supported in Amazon SageMaker. Managed Spot Training is supported\n",
    "in all AWS Regions where Amazon SageMaker is currently available.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f28f04-7ce8-4ced-9594-77ab72066ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: Which instances can I use with Managed Spot Training in SageMaker?\n",
      "[Output]: all instances supported in Amazon SageMaker\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "text_input = prompt_template.replace(\"{context}\", context).replace(\"{question}\", question)\n",
    "\n",
    "out = predictor.predict({\"inputs\": text_input})\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "print(f\"[Input]: {question}\\n[Output]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c6964-4c87-4134-8be0-6d31371094eb",
   "metadata": {},
   "source": [
    "Let's see if our LLM is capable of following our instructions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602bdd95-42b4-477a-a081-1802d6f81512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: What color is my desk?\n",
      "[Output]: I don't know\n"
     ]
    }
   ],
   "source": [
    "unanswerable_question = \"What color is my desk?\"\n",
    "\n",
    "text_input = prompt_template.replace(\"{context}\", context).replace(\"{question}\", unanswerable_question)\n",
    "\n",
    "out = predictor.predict({\"inputs\": text_input})\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "print(f\"[Input]: {unanswerable_question}\\n[Output]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d95ba68-1391-4fde-97ee-55bcddd9a92f",
   "metadata": {},
   "source": [
    "Step 4. Use RAG based approach to identify the correct documents, and use them along with prompt and question to query LLM\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "Generate embedings for each of document in the knowledge library with the MiniLM embedding model.\n",
    "Identify top K most relevant documents based on user query.\n",
    "For a query of your interest, generate the embedding of the query using the same embedding model.\n",
    "Search the indexes of top K most relevant documents in the embedding space using the SageMaker KNN algorithm.\n",
    "Use the indexes to retrieve the corresponded documents.\n",
    "Combine the retrieved documents with prompt and question and send them into LLM.\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt -- maximum sequence length of 1024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c4e2d2b-53d5-4a48-969f-361f988358c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hub_config = {\n",
    "    'HF_MODEL_ID': 'sentence-transformers/all-MiniLM-L6-v2', # model_id from hf.co/models\n",
    "    'HF_TASK': 'feature-extraction'\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub_config,\n",
    "    role=role,\n",
    "    transformers_version=\"4.6\", # transformers version used\n",
    "    pytorch_version=\"1.7\", # pytorch version used\n",
    "    py_version=\"py36\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de794c2-3512-4af8-98ed-b30e3a2d2b84",
   "metadata": {},
   "source": [
    "Then we deploy the model as we did earlier for our generative LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89bb24e5-c892-43fa-918f-7f29716458c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "encoder = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=\"minilm-demo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52361cc7-e38f-40da-84b3-b92118e89089",
   "metadata": {},
   "source": [
    "We can then create the embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cfd789e4-9309-446c-aa5d-3a268502688e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = encoder.predict({\"inputs\": [\"some text here\", \"some more text goes here too\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95273dd-7852-436e-a261-380f76277c31",
   "metadata": {},
   "source": [
    "We will see that we have two outputs (one for each of our input sentences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6cc567e9-1a7b-4a8f-ae6d-053d3494a451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1656b86f-f9c3-4be8-b47f-97f7977a0164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[0]), len(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c73e917a-769a-4356-80bd-2d2fd5d89274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8adf8a9-18d5-4751-ad51-f2ac348dbe7f",
   "metadata": {},
   "source": [
    "Perfect! There's just one problem, how do we transform these eight vector embeddings into a single sentence embedding? For this, we simply take the mean value across each vector dimension, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1a120bd-8b71-44ee-b3e9-5499b6f1fca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = np.mean(np.array(out), axis=1)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5c56c-013a-46c7-94fd-e9eed73b6e1f",
   "metadata": {},
   "source": [
    "Now we have two 384-dimensional vector embeddings, one for each of our input texts. To make our lives easier later, we will wrap this encoding process into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c4e3fd2-7271-49b2-b882-1c589c90fab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def embed_docs(docs: List[str]) -> List[List[float]]:\n",
    "    out = encoder.predict({'inputs': docs})\n",
    "    embeddings = np.mean(np.array(out), axis=1)\n",
    "    return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3547b7",
   "metadata": {},
   "source": [
    "## Configure LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb4ef122-30de-4baf-8162-cc2eab768da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 512, \"top_p\": 0.8, \"temperature\": 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29135c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        print (response_json)\n",
    "        return response_json[\"generated_texts\"][0]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549d93a1-36a6-4783-8b28-97894650b856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to get logs\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_data = {\"text_inputs\": prompt, **model_kwargs}\n",
    "        input_str = json.dumps(input_data)\n",
    "        \n",
    "        print(f\"Transformed Input: {input_str}\")\n",
    "        \n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        \n",
    "        print(f\"Transformed Output: {response_json}\")\n",
    "        \n",
    "        return response_json[\"generated_texts\"][0]\n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3d559a5-64cc-4c49-9faf-82f9a75aa910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adjust based on chatgpt suggestions\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_data = {\n",
    "            \"inputs\": prompt,  # Adjust this field based on the expected input format\n",
    "            **model_kwargs,\n",
    "        }\n",
    "        input_str = json.dumps(input_data)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "     \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        print (response_json)\n",
    "        return response_json[\"generated_texts\"][0]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fae0bb69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker-runtime\") # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=sm_client,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a7cb9",
   "metadata": {},
   "source": [
    "## Zero-shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21eec468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "As a helpful energy specialist, please answer the question, focusing on numerical data.\n",
    "Don't invent facts. If you can't provide a factual answer, say you don't know what the answer is.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(system_prompt + \"{content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80b8fd9d-0fad-4184-a680-b477da384ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(system_prompt + \"{context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28e5f4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77e58740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context =\"Solar investments in China have increased by 30% each year in the last decade\"\n",
    "question = \"What is the latest trend for solar investments in China?\"\n",
    "\n",
    "query = f\"question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "803ac882-ba65-462d-bba7-9388ac60cd92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is the latest trend for solar investments in China?\n"
     ]
    }
   ],
   "source": [
    "query = f\"question: {question}\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ba91871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'question', 'context'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:211\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    The return value of the function being wrapped.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m emit_warning()\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/chains/base.py:538\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    539\u001b[0m         _output_key\n\u001b[1;32m    540\u001b[0m     ]\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    544\u001b[0m         _output_key\n\u001b[1;32m    545\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:211\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for the original wrapped callable that emits a warning.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    The return value of the function being wrapped.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m emit_warning()\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/chains/base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    361\u001b[0m }\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/chains/base.py:138\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m include_run_info \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_run_info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    136\u001b[0m return_only_outputs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_only_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 138\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    140\u001b[0m     callbacks,\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    148\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/chains/base.py:475\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     external_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    474\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexternal_context)\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain/chains/base.py:264\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(inputs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing some input keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'question', 'context'}"
     ]
    }
   ],
   "source": [
    "answer = llm_chain.run(text_input)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7575a3d9-715d-4f38-b43d-1f9ae9b74825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is the latest trend for solar investments in China?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I don't know\"}]\n",
      "I don't know\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "from langchain_community.llms.sagemaker_endpoint import SagemakerEndpoint, LLMContentHandler\n",
    "#from langchain.chains import PromptTemplate, LLMChain\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_data = {\n",
    "            \"inputs\": prompt,  # Adjust this field based on the expected input format\n",
    "            **model_kwargs,\n",
    "        }\n",
    "        input_str = json.dumps(input_data)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: 'StreamingBody') -> str:\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            print(response_json)\n",
    "            return response_json[0][\"generated_text\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Replace 'endpoint_name' and 'model_kwargs' with your actual values\n",
    "endpoint_name = \"flan-t5-demo\"\n",
    "model_kwargs = {\"max_new_tokens\": 512, \"top_p\": 0.8, \"temperature\": 0.8}\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker-runtime\")  # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=sm_client,\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(system_prompt + \"{context}\")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "context = \"Solar investments in China have increased by 30% each year in the last decade\"\n",
    "question = \"What is the latest trend for solar investments in China?\"\n",
    "\n",
    "query = f\"question: {question}\"\n",
    "print(query)\n",
    "\n",
    "answer = llm_chain.run({\"context\": context, \"question\": query})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "982830bc-4a87-4b23-aa27-2a144a4d877f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I don't know\"}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# Create a SageMaker client\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')  # Replace 'your-region' with your AWS region\n",
    "\n",
    "# Define the input data\n",
    "text_input = {\n",
    "    \"inputs\": \"Answer the following QUESTION based on the CONTEXT\\ngiven. If you do not know the answer and the CONTEXT doesn't\\ncontain the answer truthfully say \\\"I don't know\\\".\\n\\nCONTEXT:\\nSolar investments in China have increased by 30% each year in the last decade\\n\\nQUESTION:\\nWhat is the latest trend for solar investments in China?\\n\\nANSWER:\\nSolar investments in China have increased by 30% each year in the last decade\",\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"top_p\": 0.8,\n",
    "    \"temperature\": 0.8\n",
    "}\n",
    "\n",
    "# Call the SageMaker endpoint\n",
    "try:\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName='flan-t5-demo',\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(text_input)\n",
    "    )\n",
    "\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    print(result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error raised by inference endpoint: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45079dc5",
   "metadata": {},
   "source": [
    "## RAG example with PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19eb8f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ded1aba-4832-448a-982f-942bcdff6933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1509c72-5b94-4a6d-9963-5a64edf4af15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "#this is working ,it is created as pypdf can't load s3 object directly\n",
    "import boto3\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Specify your S3 bucket name and item name\n",
    "bucket_name = \"bo-automation\"\n",
    "item_name = \"langchain-rag-demo/Coal2022.pdf\"\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Get the PDF file content from S3\n",
    "response = s3.get_object(Bucket=bucket_name, Key=item_name)\n",
    "pdf_content = response[\"Body\"].read()\n",
    "\n",
    "# Use BytesIO to create a file-like object from the PDF content\n",
    "pdf_file = BytesIO(pdf_content)\n",
    "\n",
    "# Save the contents to a temporary file\n",
    "with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "    temp_file.write(pdf_content)\n",
    "    temp_file_path = temp_file.name\n",
    "\n",
    "# Create a PyPDFLoader instance and load the PDF document\n",
    "loader = PyPDFLoader(temp_file_path)\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "\n",
    "# Now you can work with the 'document' object, which represents the PDF content\n",
    "# For example, you can access the pages: document.pages\n",
    "\n",
    "# Optionally, delete the temporary file\n",
    "os.remove(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "aa2d83ca-087e-411c-aa6f-50e31ddc3707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n",
      "Original text length: 137, number of chunks: 1701\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Specify the chunk size and overlap\n",
    "chunk_size = 180\n",
    "chunk_overlap = 0\n",
    "\n",
    "# Initialize the splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "# Assuming 'docs' is the list of loaded documents\n",
    "for document in docs:\n",
    "    # Extract text from the document\n",
    "    #text = document.content  # Adjust this based on the actual structure of your Document object\n",
    "\n",
    "    # Split the text into chunks\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # Add the chunks to the list\n",
    "    all_chunks += chunks\n",
    "\n",
    "    # Print information about the chunks\n",
    "    print(f\"Original text length: {len(docs)}, number of chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745175",
   "metadata": {},
   "source": [
    "### Analyze documents with Amazon Textract and split them in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62a98afe-48ac-41dd-b177-0515006fed98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming you have the list of URIs\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "\n",
    "# Assuming you have the list of URIs\n",
    "uris = [\"s3://bo-automation/langchain-rag-demo/Coal2022.pdf\",\"s3://bo-automation/langchain-rag-demo/WorldEnergyInvestment2023.pdf\", \"s3://bo-automation/langchain-rag-demo/WorldEnergyOutlook2023.pdf\"]\n",
    "\n",
    "for uri in uris:\n",
    "    print(f\"Loading {uri}\")\n",
    "\n",
    "    # Extract bucket name and object key from the URI\n",
    "    uri_parts = uri.split(\"/\")\n",
    "    bucket_name = uri_parts[2]\n",
    "    item_name = \"/\".join(uri_parts[3:])\n",
    "\n",
    "    # Load the PDF using s3fs and PyPDF2\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, item_name)\n",
    "    fs = obj.get()['Body'].read()\n",
    "    pdf = PdfReader(BytesIO(fs))\n",
    "\n",
    "    # Extract text using PyPDF2\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf.pages)):\n",
    "        page = pdf.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "    # Process the extracted text as needed\n",
    "    print(f\"Loaded {uri}, text length: {len(text)}\")\n",
    "    #print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c74a8",
   "metadata": {},
   "source": [
    "### Embed document chunks and store them in FAISS\n",
    "https://github.com/facebookresearch/faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "908d463e-445b-49eb-b5c9-539c790047fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fa50471b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define embedding model\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c61d096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed chunks\n",
    "#embeddings_db = FAISS.from_documents(all_chunks, embeddings)\n",
    "print(f\"Number of chunks: {len(all_chunks)}\")\n",
    "#print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "# Embed chunks\n",
    "embeddings_db = FAISS.from_documents(all_chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8f3cbb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save database\n",
    "embeddings_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb455a",
   "metadata": {},
   "source": [
    "### Shortcut : load existing embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e86b0ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22850",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4d47",
   "metadata": {},
   "source": [
    "### Configure RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4148a333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = embeddings_db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6c1db22a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "prompt_template = \"\"\"\n",
    "As a helpful energy specialist, please answer the question below, focusing on numerical data and using only the context below.\n",
    "Don't invent facts. If you can't provide a factual answer, say you don't know what the answer is.\n",
    "\n",
    "question: {question}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e503141b-56d8-4e24-bea7-6f97b9443007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#working version\n",
    "# Define prompt template1\n",
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(system_prompt + \"{context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a92ef004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt_template})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06143a",
   "metadata": {},
   "source": [
    "### Ask our question again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "64223366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'By 2025, coal demand is forecast to increase by 2. 6 Mt, despite'}]\n",
      "By 2025, coal demand is forecast to increase by 2. 6 Mt, despite\n"
     ]
    }
   ],
   "source": [
    "question = \"outlook for Global coal consumption in 2025?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "#print(answer)\n",
    "\n",
    "#answer = chain.run({\"query\": question})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8a47bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I don't know\"}]\n",
      "I don't know\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the impact of High coal prices on coal mining projects?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8797d3b-de9f-42ba-8563-1c884ea34a83",
   "metadata": {},
   "source": [
    "Alternate approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "114da2cb-c92e-4e79-8128-df13f6b03dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag_query(question: str) -> str:\n",
    "    # create query vec\n",
    "    query_vec = embed_docs(question)[0]\n",
    "    # query pinecone\n",
    "    res = index.query(query_vec, top_k=5, include_metadata=True)\n",
    "    # get contexts\n",
    "    contexts = [match.metadata['text'] for match in res.matches]\n",
    "    # build the multiple contexts string\n",
    "    context_str = construct_context(contexts=contexts)\n",
    "    # create our retrieval augmented prompt\n",
    "    text_input = prompt_template.replace(\"{context}\", context_str).replace(\"{question}\", question)\n",
    "    # make prediction\n",
    "    out = llm.predict({\"inputs\": text_input})\n",
    "    return out[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c6ec1ad-27ef-4fcb-828f-5d9a258973cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrag_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat does STEPS mean?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[66], line 5\u001b[0m, in \u001b[0;36mrag_query\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      3\u001b[0m query_vec \u001b[38;5;241m=\u001b[39m embed_docs(question)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# query pinecone\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mquery(query_vec, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, include_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# get contexts\u001b[39;00m\n\u001b[1;32m      7\u001b[0m contexts \u001b[38;5;241m=\u001b[39m [match\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mmatches]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "rag_query(\"What does STEPS mean?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d4337",
   "metadata": {},
   "source": [
    "## Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "066351f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4f083366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "response = sagemaker.list_endpoints()\n",
    "\n",
    "if not response['Endpoints']:\n",
    "    print(\"No active endpoints.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e58dd-125e-4d7c-9337-540d0ec7ce33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
