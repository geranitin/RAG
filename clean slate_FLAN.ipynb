{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e561e1b6",
   "metadata": {},
   "source": [
    "# Building a simple RAG chatbot with LangChain, Hugging Face, FAISS, Amazon SageMaker and Amazon Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e7b807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.1 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pip install sagemaker langchain amazon-textract-caller amazon-textract-textractor sentence-transformers pypdf pip install faiss-cpu -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91613d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker\n",
    "from typing import Dict\n",
    "from langchain import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ff459",
   "metadata": {},
   "source": [
    "## Deploy LLM on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4f57eb-d660-41f3-901a-ce93d32ae874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'My name is Arthur. My name is Arthur.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t5 XL\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "\t#'HF_MODEL_ID':'google/flan-t5-small',\n",
    "    #'HF_MODEL_ID':'bigscience/bloomz-3b',\n",
    "    #'HF_MODEL_ID':'google/flan-t5-xl',\n",
    "    'HF_MODEL_ID':'google-t5/t5-base',\n",
    "\t'SM_NUM_GPUS': json.dumps(1)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g4dn.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=600,\n",
    "    endpoint_name=\"flan-t5base-demo\"\n",
    "  )\n",
    "  \n",
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"Translate to German:  My name is Arthur\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bbbb01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flan-t5base-demo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90184f3-87f8-47fd-ad88-11138a52bd9b",
   "metadata": {},
   "source": [
    "**Zero Shot example** 1. Ask a question to LLM without providing the context\n",
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. Let's directly ask the model a question and see how they respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b4361a-d97f-466f-a179-6c4d71248d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Which instances can I use with Managed Spot Training in SageMaker?'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "\n",
    "out = predictor.predict({\"inputs\": question})\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1adad45c-59e5-4b90-92cd-47421f2d0ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|prompter|>|prompter|>|help|>\n"
     ]
    }
   ],
   "source": [
    "# define payload\n",
    "prompt=\"\"\"<|prompter|>How can i stay more active during winter? Give me 3 tips.<|endoftext|><|assistant|>\"\"\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 10,\n",
    "    \"min_length\": 200,\n",
    "    \"stop\": [\"<|endoftext|>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "# print(response[0][\"generated_text\"][:-len(\"<human>:\")])\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ee932-209d-4c0d-849a-2f7413d58a23",
   "metadata": {},
   "source": [
    "Step 3. Improve the answer to the same question using prompt engineering with insightful context\n",
    "To better answer the question well, we provide extra contextual information, combine it with a prompt, and send it to model together with the question. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "482f3ca1-9ef3-41e7-a8bd-1f2fab9fdb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Managed Spot Training can be used with all instances\n",
    "supported in Amazon SageMaker. Managed Spot Training is supported\n",
    "in all AWS Regions where Amazon SageMaker is currently available.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f28f04-7ce8-4ced-9594-77ab72066ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Question]: Which instances can I use with Managed Spot Training in SageMaker?\n",
      "[Answer]: Managed Spot Training is supported in all AWS regions where Amazon SageMaker is currently available\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template for T5\n",
    "prompt_template = \"\"\"summarize: CONTEXT: {context} QUESTION: {question}\"\"\"\n",
    "\n",
    "# Replace placeholders with actual context and question\n",
    "text_input = prompt_template.format(context=context, question=question)\n",
    "\n",
    "# Predict the answer using the adjusted prompt\n",
    "out = predictor.predict({\"inputs\": text_input})\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "\n",
    "# Print the question and the model's output\n",
    "print(f\"[Question]: {question}\\n[Answer]: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "395c162b-c337-49c4-b3ce-456d67075a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Summary]: data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data\n"
     ]
    }
   ],
   "source": [
    "#summarize based on context\n",
    "context = (\"Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains.[11] The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[12][13] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[14][15] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[16]\")\n",
    "\n",
    "prompt_template = \"\"\"summarize based on the CONTEXT\n",
    "given.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "text_input = prompt_template.replace(\"{context}\", context)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": text_input,\n",
    "    \"parameters\": {\n",
    "        \"min_length\": 80,\n",
    "        \"max_length\": 512\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make the prediction request\n",
    "out = predictor.predict(payload)\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "print(f\"[Summary]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c6964-4c87-4134-8be0-6d31371094eb",
   "metadata": {},
   "source": [
    "Let's see if our LLM is capable of following our instructions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "602bdd95-42b4-477a-a081-1802d6f81512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: What color is my desk?\n",
      "[Output]: Managed Spot Training is supported in all AWS regions where Amazon SageMaker is currently available\n"
     ]
    }
   ],
   "source": [
    "unanswerable_question = \"What color is my desk?\"\n",
    "\n",
    "text_input = prompt_template.replace(\"{context}\", context).replace(\"{question}\", unanswerable_question)\n",
    "\n",
    "out = predictor.predict({\"inputs\": text_input})\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "print(f\"[Input]: {unanswerable_question}\\n[Output]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d898e-9819-4b8f-a423-414535ff2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = (\"Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains.[11] The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[12][13] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[14][15] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[16]\")\n",
    "\n",
    "inputs=tokenizer.encode(\"sumarize: \" +sequence,return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "output = model.generate(inputs, min_length=80, max_length=100)\n",
    "\n",
    "summary=tokenizer.decode(output[0])\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3547b7",
   "metadata": {},
   "source": [
    "## Configure LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb4ef122-30de-4baf-8162-cc2eab768da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 1024,\n",
    "    \"top_p\": 0.8, \"temperature\": 0.8,\"max_length\": 512,\"min_length\":300 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3d559a5-64cc-4c49-9faf-82f9a75aa910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define content handler class\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_data = {\n",
    "            \"inputs\": prompt,  # Adjust this field based on the expected input format\n",
    "            **model_kwargs,\n",
    "        }\n",
    "        input_str = json.dumps(input_data)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "     \n",
    "    def transform_output(self, output: 'StreamingBody') -> str:\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            print(response_json)\n",
    "            return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fae0bb69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker-runtime\") # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=sm_client,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a7cb9",
   "metadata": {},
   "source": [
    "## Zero-shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80b8fd9d-0fad-4184-a680-b477da384ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(system_prompt + \"{context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28e5f4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77e58740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context =\"Solar investments trends in China have increased by 30% each year in the last decade\"\n",
    "question = \"What is the investment trend for solar investments in China?\"\n",
    "\n",
    "query = f\"question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "803ac882-ba65-462d-bba7-9388ac60cd92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is the investment trend for solar investments in China?\n"
     ]
    }
   ],
   "source": [
    "query = f\"question: {question}\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ba91871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'True'}]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "answer = llm_chain.run({\"context\": context, \"question\": query})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45079dc5",
   "metadata": {},
   "source": [
    "## RAG example with PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19eb8f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded1aba-4832-448a-982f-942bcdff6933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "#!pip install pypdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e586d1-21c6-4daf-8db1-87b69ad69ccb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#this is working for single pdf it is created as pypdf can't load s3 object directly\n",
    "import boto3\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Specify your S3 bucket name and item name\n",
    "bucket_name = \"bo-automation\"\n",
    "item_name = \"langchain-rag-demo/Coal2022.pdf\"\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Get the PDF file content from S3\n",
    "response = s3.get_object(Bucket=bucket_name, Key=item_name)\n",
    "pdf_content = response[\"Body\"].read()\n",
    "\n",
    "# Use BytesIO to create a file-like object from the PDF content\n",
    "pdf_file = BytesIO(pdf_content)\n",
    "\n",
    "# Save the contents to a temporary file\n",
    "with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "    temp_file.write(pdf_content)\n",
    "    temp_file_path = temp_file.name\n",
    "\n",
    "# Create a PyPDFLoader instance and load the PDF document\n",
    "loader = PyPDFLoader(temp_file_path)\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "\n",
    "# Now you can work with the 'document' object, which represents the PDF content\n",
    "# For example, you can access the pages: document.pages\n",
    "\n",
    "# Optionally, delete the temporary file\n",
    "os.remove(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35deeb8a-0073-464e-84d2-16d4170e773a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: langchain-rag-demo/5G_Dimensioning and Network Design Guidelines.docx.pdf\n",
      "91\n",
      "Loading file: langchain-rag-demo/dimensioning_guide.pdf\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#working fine to load multiple files\n",
    "import boto3\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Initialize Boto3 S3 client\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = \"bo-automation1\"\n",
    "prefix = \"langchain-rag-demo/\"\n",
    "\n",
    "# List objects within the specified directory\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# Initialize a list to hold all documents\n",
    "all_docs = []\n",
    "\n",
    "# Iterate over each file and load its contents\n",
    "for obj in response.get('Contents', []):\n",
    "    item_name = obj['Key']\n",
    "    # Process only PDF files\n",
    "    if item_name.endswith('.pdf'):\n",
    "        print(f\"Loading file: {item_name}\")\n",
    "\n",
    "        # Get the PDF file content from S3\n",
    "        pdf_response = s3.get_object(Bucket=bucket_name, Key=item_name)\n",
    "        pdf_content = pdf_response[\"Body\"].read()\n",
    "\n",
    "        # Use BytesIO to create a file-like object from the PDF content\n",
    "        pdf_file = BytesIO(pdf_content)\n",
    "\n",
    "        # Save the contents to a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "            temp_file.write(pdf_content)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        # Load the PDF document\n",
    "        loader = PyPDFLoader(temp_file_path)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(len(docs))\n",
    "\n",
    "        # Optionally, delete the temporary file\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "# Now all_docs contains documents from all PDF files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a38dfb43-ae83-4355-8c37-56369ad7cb99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "print(len(all_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef19880-00ae-479c-8e8a-6f57b8e73348",
   "metadata": {
    "tags": []
   },
   "source": [
    "#not using,but working fine\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Print information about the chunks\n",
    "print(f\"Original text length: {len(docs)}, number of chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7250dfde-4f8e-47cc-ae87-803fec118093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Original text length: 127, number of chunks: 745\n",
      "Number of chunks: 94615\n"
     ]
    }
   ],
   "source": [
    "#working fine\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "all_chunks = []\n",
    "\n",
    "# Assuming 'docs' is the list of loaded documents\n",
    "for document in all_docs:\n",
    "    # Extract text from the document\n",
    "    #text = document.content  # Adjust this based on the actual structure of your Document object\n",
    "\n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "    # Add the chunks to the list\n",
    "    all_chunks += chunks\n",
    "\n",
    "    # Print information about the chunks\n",
    "    print(f\"Original text length: {len(all_docs)}, number of chunks: {len(chunks)}\")\n",
    "\n",
    "#chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Print information about the chunks\n",
    "#print(f\"Original text length: {len(docs)}, number of chunks: {len(chunks)}\")\n",
    "print(f\"Number of chunks: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745175",
   "metadata": {},
   "source": [
    "### Analyze documents with Amazon Textract and split them in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c74a8",
   "metadata": {},
   "source": [
    "### Embed document chunks and store them in FAISS\n",
    "https://github.com/facebookresearch/faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "908d463e-445b-49eb-b5c9-539c790047fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa50471b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73090c0d380f47079b21cdd89da0cbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a21e4b8bf24443ba3d8ef15008b5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f27733c8214bc0af75680897a9702d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ebb39910e741f492d9408b4220caf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bbdc8787754a28893144e9884dc4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f294a14c6a0c4f6ca5b25a1ac614a85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1709091958be45fcb6b8e6dbd82aabeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c1cd8b44d64bb687e2dd6ac6f59506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7c02f2958c4be094e070d00822c1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adde06e17cb64b32a78097dee94d2ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419b275524534fe09654185ec25a0bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define embedding model\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c61d096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 94615\n",
      "CPU times: user 2min 31s, sys: 4.91 s, total: 2min 36s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed chunks\n",
    "#embeddings_db = FAISS.from_documents(all_chunks, embeddings)\n",
    "print(f\"Number of chunks: {len(all_chunks)}\")\n",
    "#print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "# Embed chunks\n",
    "embeddings_db = FAISS.from_documents(all_chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eda69b-0567-41e7-8475-3c699f9b18fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#not using\n",
    "%%time\n",
    "#alternate\n",
    "# Embed chunks\n",
    "#embeddings_db = FAISS.from_documents(all_chunks, embeddings)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "#print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "# Embed chunks\n",
    "embeddings_db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f3cbb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save database\n",
    "embeddings_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb455a",
   "metadata": {},
   "source": [
    "### Shortcut : load existing embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e86b0ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22850",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4d47",
   "metadata": {},
   "source": [
    "### Configure RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4148a333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = embeddings_db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e503141b-56d8-4e24-bea7-6f97b9443007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#working version\n",
    "# Define prompt template1\n",
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(system_prompt + \"{context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a92ef004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt_template})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06143a",
   "metadata": {},
   "source": [
    "### Ask our question again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64223366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Contents 1 Introduction to Dimensioning Guide 1 1.1 System Requirements for Cloud Container Distribution'}]\n",
      "Contents 1 Introduction to Dimensioning Guide 1 1.1 System Requirements for Cloud Container Distribution\n"
     ]
    }
   ],
   "source": [
    "question = \"System Requirements for Cloud Container Distribution?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "#print(answer)\n",
    "\n",
    "#answer = chain.run({\"query\": question})\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8a47bea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Ceph does not support filling up 100% of its raw storage so distribution. Also,'}]\n",
      "Ceph does not support filling up 100% of its raw storage so distribution. Also,\n"
     ]
    }
   ],
   "source": [
    "question = \"How much raw capacity is available in ceph cluster?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4bcbacd-e9f5-4169-84ee-60d34e8f1732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'if you do not know the answer truthfully say \"I don\\'t know\".'}]\n",
      "if you do not know the answer truthfully say \"I don't know\".\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the result of encryption?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9732d7c9-b848-4399-be50-8d3f441db889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'a maximum of 6+1 QUESTION: A CCSM instance can be connected'}]\n",
      "a maximum of 6+1 QUESTION: A CCSM instance can be connected\n"
     ]
    }
   ],
   "source": [
    "question = \"A CCSM instance can be connected to a maximum of how many HSM?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "272eb5bb-901f-4b3f-b1fc-60f4335dabd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'based on the CONTEXT given. 1.1 System Requirements for'}]\n",
      "based on the CONTEXT given. 1.1 System Requirements for\n"
     ]
    }
   ],
   "source": [
    "question = \"what are System Requirements for Cloud Container Distributionl\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8797d3b-de9f-42ba-8563-1c884ea34a83",
   "metadata": {},
   "source": [
    "Alternate approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc05de-cdb2-4370-bf04-c54104d07aaa",
   "metadata": {},
   "source": [
    "Test interface using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bf98bef-bb38-4212-9315-4b508f3b7782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic version: 2.6.2\n"
     ]
    }
   ],
   "source": [
    "#pip install gradio\n",
    "#import gradio\n",
    "import pydantic\n",
    "\n",
    "#print(\"Gradio version:\", gradio.__version__)\n",
    "print(\"Pydantic version:\", pydantic.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b48b1e18-7afc-422d-a3fc-c828cbb393cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.6.2)\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.19.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting fastapi (from gradio)\n",
      "  Downloading fastapi-0.109.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.10.1 (from gradio)\n",
      "  Downloading gradio_client-0.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (6.1.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (1.22.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.9.15)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (10.0.1)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.27.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio-client==0.10.1->gradio) (2023.10.0)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==0.10.1->gradio)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n",
      "Requirement already satisfied: toolz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.0.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.4)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.6.0)\n",
      "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
      "  Downloading starlette-0.36.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.18)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\n",
      "Downloading gradio-4.19.2-py3-none-any.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.10.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5582 sha256=4eac96c7ebadaa7b63713a256a27b25f873454dd0b46f578f6c74ec62cb9ab87\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, typer, tomlkit, shellingham, semantic-version, ruff, python-multipart, h11, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, altair, gradio\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.12.1\n",
      "    Uninstalling tomlkit-0.12.1:\n",
      "      Successfully uninstalled tomlkit-0.12.1\n",
      "Successfully installed aiofiles-23.2.1 altair-5.2.0 fastapi-0.109.2 ffmpy-0.3.2 gradio-4.19.2 gradio-client-0.10.1 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.2.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 typer-0.9.0 uvicorn-0.27.1 websockets-11.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pydantic gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f0c68e8-d96b-4a8f-ba5e-56b16080526a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.6.2)\n",
      "Requirement already satisfied: gradio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.19.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (5.2.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.109.2)\n",
      "Requirement already satisfied: ffmpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.10.1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (6.1.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (1.22.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (3.9.15)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (10.0.1)\n",
      "Requirement already satisfied: pydub in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio) (0.27.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio-client==0.10.1->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio-client==0.10.1->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n",
      "Requirement already satisfied: toolz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.0.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.4)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.6.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->gradio) (0.36.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.18)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pydantic gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93d34bf9-98a8-4ed8-bc5f-12ed79f929dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio==3.48.0\n",
      "  Downloading gradio-3.48.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (5.2.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.109.2)\n",
      "Requirement already satisfied: ffmpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.3.2)\n",
      "Collecting gradio-client==0.6.1 (from gradio==3.48.0)\n",
      "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (6.1.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (1.22.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (3.9.15)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (10.0.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.6.2)\n",
      "Requirement already satisfied: pydub in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (6.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.31.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (4.8.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (0.27.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio==3.48.0) (11.0.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gradio-client==0.6.1->gradio==3.48.0) (2023.10.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.48.0) (4.19.1)\n",
      "Requirement already satisfied: toolz in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.48.0) (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio==3.48.0) (3.12.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio==3.48.0) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.48.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.48.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.48.0) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.48.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.48.0) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests~=2.0->gradio==3.48.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests~=2.0->gradio==3.48.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests~=2.0->gradio==3.48.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests~=2.0->gradio==3.48.0) (2023.7.22)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.48.0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.48.0) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->gradio==3.48.0) (0.36.3)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio==3.48.0) (4.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio==3.48.0) (1.0.4)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->gradio==3.48.0) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.48.0) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx->gradio==3.48.0) (1.1.3)\n",
      "Downloading gradio-3.48.0-py3-none-any.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gradio-client, gradio\n",
      "  Attempting uninstall: gradio-client\n",
      "    Found existing installation: gradio_client 0.10.1\n",
      "    Uninstalling gradio_client-0.10.1:\n",
      "      Successfully uninstalled gradio_client-0.10.1\n",
      "  Attempting uninstall: gradio\n",
      "    Found existing installation: gradio 4.19.2\n",
      "    Uninstalling gradio-4.19.2:\n",
      "      Successfully uninstalled gradio-4.19.2\n",
      "Successfully installed gradio-3.48.0 gradio-client-0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio==3.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f05232d-8997-4191-a5ad-680dcb24ea43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: starlette in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.36.3)\n",
      "Collecting starlette\n",
      "  Downloading starlette-0.37.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from starlette) (4.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette) (1.1.3)\n",
      "Downloading starlette-0.37.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: starlette\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.36.3\n",
      "    Uninstalling starlette-0.36.3:\n",
      "      Successfully uninstalled starlette-0.36.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.109.2 requires starlette<0.37.0,>=0.36.3, but you have starlette 0.37.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed starlette-0.37.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade starlette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e9cb79d-c5cc-4bbd-90c2-2ac71ab2a3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a79a1283-1157-4779-825c-21aa72f594f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.23.25-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.23.22 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.23.25-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.23.25 PyMuPDFb-1.23.22\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54f85134-3888-41f0-9340-101bb318ad11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming the initialization of your model, prompt template, and chain is done here\n",
    "# Define prompt template\n",
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(system_prompt + \"{context}\")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# Define gradio model function\n",
    "def model_function(question):\n",
    "    # Make prediction using the chain\n",
    "    answer = chain.run({\"query\": question})\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f60bc014-cb3d-4aa5-bdb6-feb677d93796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Sagemaker notebooks may require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Running on public URL: https://c9f7c373ff78f49475.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c9f7c373ff78f49475.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'CCPC ANSWER: What is CCPC ANSWER: What'}]\n",
      "[{'generated_text': 'CCSM: if you do not know the answer truthfully say \"I don\\'t'}]\n"
     ]
    }
   ],
   "source": [
    "# Define the gradio interface for our use case\n",
    "interface = gr.Interface(fn=model_function, inputs=\"text\", outputs=\"text\")\n",
    "interface.launch()\n",
    "#interface.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836674f9-ccc9-4984-b9e1-183d1f5e92d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test gradio 2\n",
    "import gradio as gr\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")\n",
    "\n",
    "# Assuming the initialization of your model, prompt template, and chain is done here\n",
    "# Define prompt template\n",
    "system_prompt = \"\"\"Answer the following QUESTION based on the CONTEXT\n",
    "given. If you do not know the answer and the CONTEXT doesn't\n",
    "contain the answer truthfully say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(system_prompt + \"{context}\")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# Initialize your text splitter with the desired chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=50)\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Function to extract text from PDF\n",
    "    text = \"\"\n",
    "    with fitz.open(stream=pdf_file.read(), filetype=\"pdf\") as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def process_text_into_chunks(text):\n",
    "    # Function to split text into chunks\n",
    "    chunks = text_splitter.split_documents([text])\n",
    "    return chunks\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    # Assuming you have a function to generate embeddings for text chunks\n",
    "    embeddings_db = [FAISS.from_documents(chunk, embeddings) for chunk in chunks] \n",
    "    # Pseudocode\n",
    "    embeddings_db.save_local(\"faiss_index\")\n",
    "\n",
    "    embeddings_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "    retriever = embeddings_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    return embeddings_db\n",
    "\n",
    "def model_function(pdf_file, question):\n",
    "    # Process the PDF and extract text\n",
    "    context = extract_text_from_pdf(pdf_file)\n",
    "    \n",
    "    # Split the context into chunks\n",
    "    chunks = process_text_into_chunks(context)\n",
    "    \n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "    \n",
    "    # Use the RAG model to answer the question based on the embeddings\n",
    "    # This part of the code would depend on how your RAG model uses embeddings to generate an answer\n",
    "    answer = chain.run({\"query\": question})\n",
    "    return answer\n",
    "\n",
    "# Gradio Interface\n",
    "interface = gr.Interface(\n",
    "    fn=model_function, \n",
    "    inputs=[gr.File(type=\"file\", label=\"Upload PDF\"), gr.Textbox(label=\"Your Question\")], \n",
    "    outputs=\"text\"\n",
    ")\n",
    "\n",
    "interface.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d4337",
   "metadata": {},
   "source": [
    "## Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02b22853-674e-4e2f-8252-2653d54022a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8de28-b64c-40b6-8974-0fd2a6c9d0ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "response = sagemaker.list_endpoints()\n",
    "\n",
    "if not response['Endpoints']:\n",
    "    print(\"No active endpoints.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e58dd-125e-4d7c-9337-540d0ec7ce33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
