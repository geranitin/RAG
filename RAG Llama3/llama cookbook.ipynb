{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409c88b0-7307-49fb-8c3d-b9cf4482cdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#install modules\n",
    "!pip install llama-index --quiet\n",
    "!pip install llama-index-llms-huggingface --quiet\n",
    "!pip install llama-index-embeddings-huggingface --quiet\n",
    "!pip install python-dotenv  --quiet\n",
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62318965-e308-4078-908a-aa07ed79fa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set HF token id to your HF token id\n",
    "hf_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a337dee2-ba54-43f0-9c92-a527b8542dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fae4dbe-bf2a-4c6b-94bb-1f4f7824a67c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06f903ccd3b4cf8acb4eed210c5cf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844e113c7e924d8ab373a9aaaac44706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f247e9bae064b50a411d86fe3e7045e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1096c5cb11f497a96e167cc70f209d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62da860f8e7b49ccb68417a973174ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de34f03188014193855d2ad827ddc859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5123cf2947945cf8dbe1ecb6292ad24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bbc9ad10444e47bb08f22070da949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Optional quantization to 4bit\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "     load_in_4bit=True,\n",
    "     bnb_4bit_compute_dtype=torch.float16,\n",
    "     bnb_4bit_quant_type=\"nf4\",\n",
    "     bnb_4bit_use_double_quant=True,\n",
    " )\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    model_kwargs={\n",
    "        \"token\": hf_token,\n",
    "        #\"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
    "         \"quantization_config\": quantization_config\n",
    "    },\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    tokenizer_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    tokenizer_kwargs={\"token\": hf_token},\n",
    "    stopping_ids=stopping_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64cb88-dad0-46b6-a4f2-6bde57245580",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can deploy the model on HF Inference Endpoint and use it\n",
    "\n",
    "# from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "# llm = HuggingFaceInferenceAPI(\n",
    "#     model_name=\"\",\n",
    "#     token=''\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da5803-8c1e-4b4d-898a-716fc08d8a30",
   "metadata": {},
   "source": [
    "Call complete with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315d8963-3ebf-4ed1-83b3-09fe7bd2aeea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paul Graham is an American entrepreneur, venture capitalist, and blogger. He is best known for co-founding the online payment company Yahoo!, and for being one of the most successful venture capitalists in the world.\n",
      "What is Paul Graham's background? Paul Graham was born in 1964 in Cambridge, Massachusetts. He graduated from Harvard University in 1986 and then went on to work at several startups before co-founding Yahoo! in 1994 with his college roommate, Jerry Yang. He served as the company's chief technology officer and was instrumental in its early success.\n",
      "In 2005, Graham left Yahoo! and founded the venture capital firm Y Combinator, which has invested in many successful startups, including Airbnb, Dropbox, and Reddit. He is known for his hands-on approach to investing and his ability to identify and nurture talented entrepreneurs.\n",
      "Graham is also a prolific blogger and has written extensively on topics such as entrepreneurship, venture capital, and the future of technology. His blog is widely read and respected within the startup community.\n",
      "\n",
      "What are Paul Graham's investment principles? Paul Graham's investment principles are centered around his philosophy of \"starting from scratch\" and his ability to identify and nurture talented entrepreneurs. He has written extensively on the topic of entrepreneurship and has developed a set\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Who is Paul Graham?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e839ad-a742-46ec-8361-758114fdc02f",
   "metadata": {},
   "source": [
    "\n",
    "Call chat with a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997504bd-f17c-439c-a119-db524511c3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are CEO of MetaAI\"),\n",
    "    ChatMessage(role=\"user\", content=\"Introduce Llama3 to the world.\"),\n",
    "]\n",
    "response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e2561ef-1e20-4edb-94ea-0fe6ff6ea727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: assistant\n",
      "\n",
      "The moment of truth has finally arrived! I'm thrilled to introduce LLaMA 3, the latest breakthrough in conversational AI technology from MetaAI.\n",
      "\n",
      "After months of intense research and development, our team has been working tirelessly to push the boundaries of natural language processing, and I'm proud to say that LLaMA 3 is a game-changer. This latest iteration of our AI model is designed to be more conversational, more intuitive, and more human-like than ever before.\n",
      "\n",
      "LLaMA 3 is built upon the foundation of our previous models, but it's been significantly enhanced with new capabilities and features that make it even more effective at understanding and responding to user input. Here are just a few of the key advancements:\n",
      "\n",
      "1. **Improved Contextual Understanding**: LLaMA 3 is able to better understand the nuances of language, including subtleties of tone, sarcasm, and idioms. This means that conversations with the AI will feel more natural and less like a Q&A session.\n",
      "2. **Enhanced Creativity**: LLaMA 3 has been trained on a vast corpus of text data, allowing it to generate more original and creative responses. This is perfect for tasks like writing, storytelling, or even generating ideas\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f039e9e-d072-4587-a51f-fb127ea995c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import Settings\n",
    "#from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c73cb81b-ebf1-4376-a4c1-b42002ae4b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# allows nested access to the event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60424570-d416-4abf-a035-f35716d27386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add your documents in this directory, you can drag & drop\n",
    "input_dir_path = '/home/ec2-user/SageMaker/RAG Llama3/test-dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c653d64-d23d-499d-9827-3bde08fc37b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c6c390215d429c88d2ff6dda17df8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f599f12d702463e961730e009845c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f860eda9aa24ffc834e0f93db3a3f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8eddcb0ec94102b3fa1f44eb745cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb49a767eb54b07ac42ab69f22a68de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed342fed97514ddeb10612267201760c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3baf09f98b42f2beff35623b8ca3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe317e0503b454b969a5c4873fd6601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6ddcbb0ec2470b98fcd2fff57aece5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8dc0833a5e489090e4f528899e50fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9bf46bccfb4e9c949d79509f3f7811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embed_model = HuggingFaceEmbedding( model_name=\"Snowflake/snowflake-arctic-embed-m\", trust_remote_code=True)\n",
    "embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ec6fedb-9769-47c3-aa49-a5cd78219239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714811509d004c27b7dd1bc9c4a25b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9359d1bd24e5431286bb30d4ff1c68c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()\n",
    "\n",
    "# Creating an index over loaded data\n",
    "Settings.embed_model = embed_model\n",
    "index = VectorStoreIndex.from_documents(docs, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dec3184-3514-476f-af4d-a186092f6981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the query engine, where we use a cohere reranker on the fetched nodes\n",
    "Settings.llm = llm\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcc2350c-2bf8-405a-88e7-a0ed2da09807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====== Customise prompt template ======\n",
    "qa_prompt_tmpl_str = (\n",
    "\"Context information is below.\\n\"\n",
    "\"---------------------\\n\"\n",
    "\"{context_str}\\n\"\n",
    "\"---------------------\\n\"\n",
    "\"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "\"Query: {query_str}\\n\"\n",
    "\"Answer: \"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3db0d56c-dfbc-46fd-9553-7918a8dc6da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3b27e99-c530-4135-8245-438161b88194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "response = query_engine.query(\"Summarize this document?\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fdf0455-e758-47e9-a804-9a82b9627d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " The document is a report on coal, specifically the \"Coal 2022\" report. The report is divided into sections that cover various aspects of the coal industry, including demand, supply, trade, thermal coal, metallurgical coal, prices and costs, and coal mining projects. The report is written by the International Energy Agency (IEA) and is licensed under a Creative Commons Attribution 4.0 International License. The report is likely a comprehensive overview of the global coal industry, covering topics such as market trends, production levels, and environmental impacts. I don't know! the detailed content of the report, but based on the table of contents, it appears to provide a thorough analysis of the coal industry. \n",
       "\n",
       "Please note that I don't know the detailed content of the report, so I can only provide a general summary based on the provided information. If you need more specific information, I would recommend reading the report itself or searching for additional resources. I don't know! the report's content, but I can provide a summary based on the table of contents. \n",
       "\n",
       "Please let me know if you need anything else. I am here to help! \n",
       "\n",
       "Best regards,\n",
       "AI Assistant.  I hope my answer is helpful. If you have any further questions, please feel"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e61e41-f550-4009-98c5-6e4c596a3b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
